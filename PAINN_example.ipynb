{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8dd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import schnetpack as spk\n",
    "import schnetpack.representation as rep\n",
    "import schnetpack.task as task\n",
    "import schnetpack.transform as trn\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from nablaDFT.dataset import NablaDFT\n",
    "from nablaDFT.painn.train_painn import AtomisticTaskFixed, seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6f8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dataset_train_2k'  # Name of the training dataset\n",
    "datapath = 'database'              # Path to the selected dataset\n",
    "logspath = 'logs'                  # Path to log files\n",
    "nepochs = 5                        # Number of epochs to train for\n",
    "seed = 1799                        # Random seed number for reproducibility\n",
    "batch_size = 100                   # Size of each batch for training\n",
    "n_interactions = 6                 # Number of interactions to consider between atoms\n",
    "n_atom_basis = 128                 # Number of basis functions for atoms in the representation\n",
    "n_rbf = 20                         # Number of radial basis functions in the representation\n",
    "cutoff = 5.0                       # Cutoff distance (in Angstroms) for computing interactions\n",
    "devices = 1                        # Number of GPU/TPU/CPU devices to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c047086",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed)\n",
    "workpath = logspath\n",
    "\n",
    "if not os.path.exists(workpath):\n",
    "    os.makedirs(workpath)\n",
    "\n",
    "data = NablaDFT(\"ASE\", dataset_name,\n",
    "                datapath=datapath,\n",
    "                data_workdir=workpath,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=4,\n",
    "                transforms=[\n",
    "                    trn.ASENeighborList(cutoff=cutoff),\n",
    "                    trn.RemoveOffsets(\"energy\", remove_mean=True,\n",
    "                                      remove_atomrefs=False),\n",
    "                    trn.CastTo32()\n",
    "                ],\n",
    "                split_file=os.path.join(workpath, \"split.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a2371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2tb/rumiantsev/anaconda3/envs/check/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "100%|███████████████████████████████████████████| 52/52 [00:03<00:00, 15.45it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                   | Params\n",
      "---------------------------------------------------\n",
      "0 | model   | NeuralNetworkPotential | 1.2 M \n",
      "1 | outputs | ModuleList             | 0     \n",
      "---------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.628     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2tb/rumiantsev/anaconda3/envs/check/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 100. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a28eecf71fe4ecc90bfa4e9cf86b86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2tb/rumiantsev/anaconda3/envs/check/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:362: LightningDeprecationWarning: The NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0. The `AtomisticTask.optimizer_step()` hook is overridden, including the `using_native_amp` argument. Removing this argument will avoid this message, you can expect it to return True.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2tb/rumiantsev/anaconda3/envs/check/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 76. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairwise_distance = spk.atomistic.PairwiseDistances()\n",
    "radial_basis = spk.nn.radial.GaussianRBF(\n",
    "    n_rbf=n_rbf,\n",
    "    cutoff=cutoff\n",
    ")\n",
    "cutoff_fn = spk.nn.cutoff.CosineCutoff(cutoff)\n",
    "representation = rep.PaiNN(\n",
    "    n_interactions=n_interactions,\n",
    "    n_atom_basis=n_atom_basis,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=cutoff_fn\n",
    ")\n",
    "pred_energy = spk.atomistic.Atomwise(\n",
    "    n_in=representation.n_atom_basis,\n",
    "    output_key=\"energy\"\n",
    ")\n",
    "postprocessors = [\n",
    "    trn.CastTo64(),\n",
    "    trn.AddOffsets(\"energy\", add_mean=True)\n",
    "]\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=representation,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_energy],\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "output_energy = spk.task.ModelOutput(\n",
    "    name=\"energy\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1,\n",
    "    metrics={\"MAE\": torchmetrics.MeanAbsoluteError()}\n",
    ")\n",
    "\n",
    "scheduler_args = {\n",
    "    \"factor\": 0.8,\n",
    "    \"patience\": 10,\n",
    "    \"min_lr\": 1e-06\n",
    "}\n",
    "\n",
    "task = spk.task.AtomisticTask(\n",
    "    model=nnpot,\n",
    "    outputs=[output_energy],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": 1e-4},\n",
    "    scheduler_cls=ReduceLROnPlateau,\n",
    "    scheduler_args=scheduler_args,\n",
    "    scheduler_monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "# create trainer\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=workpath)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        model_path=os.path.join(workpath, \"best_inference_model\"),\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=devices,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=workpath,\n",
    "    max_epochs=nepochs,\n",
    ")\n",
    "\n",
    "trainer.fit(task, datamodule=data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d64ff-541c-4b26-8cca-cd80e4287ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = -1\n",
    "\n",
    "if gpu == -1:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.cuda.device(gpu)\n",
    "\n",
    "best_model = torch.load(os.path.join(workpath, 'best_inference_model'))\n",
    "best_model = best_model.cuda()\n",
    "best_model = best_model.eval()\n",
    "\n",
    "metric = torchmetrics.MeanAbsoluteError()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in tqdm.tqdm(data.dataset.val_dataloader()):\n",
    "\n",
    "        for k in x:\n",
    "            if x[k].dtype == torch.float64:\n",
    "                x[k] = x[k].float()\n",
    "            x[k] = x[k].to(\"cuda:0\")\n",
    "\n",
    "        target = x['energy'].cpu().clone()\n",
    "        prediction = best_model(x)['energy'].cpu()\n",
    "        mae = metric(prediction, target)\n",
    "\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07b81b-a745-468c-952c-ba7565aba048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
