_target_: nablaDFT.equiformer_v2.EquiformerV2_OC20_Lightning

model_name: "Equiformer-v2"
net:
  _target_: nablaDFT.equiformer_v2.EquiformerV2_OC20
  otf_graph: true
  num_layers: 12
  sphere_channels: 128
  attn_hidden_channels: 64
  num_heads: 8
  attn_alpha_channels: 64
  attn_value_channels: 16
  ffn_hidden_channels: 128
  norm_type: 'layer_norm_sh'    # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']
  lmax_list: [6]
  mmax_list: [2]
#  grid_resolution: 18              # [18, 16, 14, None] For `None`, simply comment this line.
  num_sphere_samples: 128
  edge_channels: 128
  use_atom_edge_embedding: true
  share_atom_edge_embedding: false         # If `True`, `use_atom_edge_embedding` must be `True` and the atom edge embedding will be shared across all blocks.
  distance_function: 'gaussian'
  num_distance_basis: 512           # not used

  attn_activation: 'silu'
  use_s2_act_attn: false       # [False, True] Switch between attention after S2 activation or the original EquiformerV1 attention.
  use_attn_renorm: true        # Attention re-normalization. Used for ablation study.
  ffn_activation: 'silu'      # ['silu', 'swiglu']
  use_gate_act: false       # [True, False] Switch between gate activation and S2 activation
  use_grid_mlp: true        # [False, True] If `True`, use projecting to grids and performing MLPs for FFNs.
  use_sep_s2_act: true        # Separable S2 activation. Used for ablation study.

  alpha_drop: 0.1         # [0.0, 0.1]
  drop_path_rate: 0.05        # [0.0, 0.05]
  proj_drop: 0.0

  weight_init: 'uniform'    # ['uniform', 'normal']

  regress_forces: true
  use_pbc: false
  max_neighbors: 30
  max_radius: 12.0
  max_num_elements: 65

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0004
  weight_decay: 0.001

lr_scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR
  _partial_: true
  lr_lambda:
      _target_: nablaDFT.equiformer_v2.lr_scheduler.CosineLRLambda
      scheduler_params:
        warmup_factor: 0.2
        warmup_epochs: 0.1
        epochs: 1000
        lr_min_factor: 0.01

losses:
  energy:
    _target_: torch.nn.L1Loss
  forces:
    _target_: nablaDFT.gemnet_oc.loss.L2Loss
loss_coefs:
  energy: 2.0
  forces: 100.0

metric:
  _target_: torchmetrics.MultitaskWrapper
  _convert_: all
  task_metrics:
    energy:
      _target_: torchmetrics.MeanAbsoluteError
    forces:
      _target_: torchmetrics.MeanAbsoluteError
