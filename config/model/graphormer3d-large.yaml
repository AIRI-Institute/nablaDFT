_target_: nablaDFT.graphormer.Graphormer3DLightning
_convert_: all

model_name: "Graphormer3D-large"
net:
  _target_: nablaDFT.graphormer.Graphormer3D
  blocks: 4
  layers: 24
  embed_dim: 1024
  ffn_embed_dim: 1024
  attention_heads: 32
  input_dropout: 0.1
  dropout: 0.1
  attention_dropout: 0.0
  activation_dropout: 0.1
  num_kernel: 128

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 3e-4
  weight_decay: 1e-3

lr_scheduler:
  _target_: nablaDFT.schedulers.get_linear_schedule_with_warmup
  _partial_: true
  num_warmup_steps: ${warmup_steps}
  num_training_steps: ${max_steps}

loss:
  _target_: torch.nn.L1Loss

metric:
  _target_: torchmetrics.MultitaskWrapper
  _convert_: all
  task_metrics:
    energy:
      _target_: torchmetrics.MeanAbsoluteError
    forces:
      _target_: torchmetrics.MeanAbsoluteError

warmup_steps: ${warmup_steps}
energy_loss_coef: 1.0
forces_loss_coef: 1.0