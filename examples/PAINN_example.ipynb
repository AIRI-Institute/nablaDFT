{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f3864e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train/test cycles example using PaiNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4edcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/atomistic-machine-learning/schnetpack/blob/master/examples/tutorials/tutorial_02_qm9.ipynb\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "import schnetpack as spk\n",
    "import schnetpack.representation as rep\n",
    "import schnetpack.task as task\n",
    "import schnetpack.transform as trn\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from nablaDFT.dataset import ASENablaDFT\n",
    "from nablaDFT.dataset.split import TestSplit\n",
    "from nablaDFT.ase_model import AtomisticTaskFixed\n",
    "from nablaDFT.utils import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a01bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dataset_train_2k'  # Name of the training dataset\n",
    "datapath = 'database'              # Path to the selected dataset\n",
    "logspath = 'logs'                  # Path to log files\n",
    "nepochs = 200                      # Number of epochs to train for\n",
    "seed = 1799                        # Random seed number for reproducibility\n",
    "batch_size = 32                    # Size of each batch for training\n",
    "train_ratio = 0.9                  # Part of dataset used for training\n",
    "val_ratio = 0.1                    # Part of dataset used for validation\n",
    "n_interactions = 6                 # Number of interactions to consider between atoms\n",
    "n_atom_basis = 128                 # Number of basis functions for atoms in the representation\n",
    "n_rbf = 20                         # Number of radial basis functions in the representation\n",
    "cutoff = 5.0                       # Cutoff distance (in Bohr) for computing interactions\n",
    "devices = 1                        # Number of GPU/TPU/CPU devices to use for training\n",
    "transforms = [\n",
    "    trn.ASENeighborList(cutoff=cutoff),\n",
    "    trn.RemoveOffsets(\"energy\", remove_mean=True, remove_atomrefs=False),\n",
    "    trn.CastTo32(),\n",
    "]                                  # data transforms used for training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498b248",
   "metadata": {},
   "source": [
    "## Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a822721",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed)\n",
    "workpath = logspath\n",
    "\n",
    "if not os.path.exists(workpath):\n",
    "    os.makedirs(workpath)\n",
    "\n",
    "datamodule = ASENablaDFT(\n",
    "    \"train\",\n",
    "    dataset_name=\"dataset_train_2k\",\n",
    "    datapath=\"database\",\n",
    "    data_workdir=logspath,\n",
    "    batch_size=batch_size,\n",
    "    train_ratio=train_ratio,\n",
    "    val_ratio=val_ratio,\n",
    "    num_workers=4,\n",
    "    train_transforms=transforms,\n",
    "    val_transforms=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208fcef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initializing training procedure and starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "028a4f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Downloading split: dataset_train_2k: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50.8M/50.8M [00:00<00:00, 186MB/s]\n",
      "Missing logger folder: logs/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type                   | Params\n",
      "---------------------------------------------------\n",
      "0 | model   | NeuralNetworkPotential | 1.2 M \n",
      "1 | outputs | ModuleList             | 0     \n",
      "---------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.628     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acae7aba429f4226b2eee24b93c3f8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 342: 'val_loss' reached 0.08478 (best 0.08478), saving model to '/mnt/2tb/ber/nablaDFT/examples/logs/checkpoints/Painn-epoch=000_val_loss=0.084779.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 684: 'val_loss' reached 0.02308 (best 0.02308), saving model to '/mnt/2tb/ber/nablaDFT/examples/logs/checkpoints/Painn-epoch=001_val_loss=0.023082.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "pairwise_distance = spk.atomistic.PairwiseDistances()\n",
    "radial_basis = spk.nn.radial.GaussianRBF(\n",
    "    n_rbf=n_rbf,\n",
    "    cutoff=cutoff\n",
    ")\n",
    "cutoff_fn = spk.nn.cutoff.CosineCutoff(cutoff)\n",
    "representation = rep.PaiNN(\n",
    "    n_interactions=n_interactions,\n",
    "    n_atom_basis=n_atom_basis,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=cutoff_fn\n",
    ")\n",
    "pred_energy = spk.atomistic.Atomwise(\n",
    "    n_in=representation.n_atom_basis,\n",
    "    output_key=\"energy\"\n",
    ")\n",
    "pred_forces = spk.atomistic.Forces()\n",
    "postprocessors = [\n",
    "    trn.AddOffsets(\"energy\", add_mean=True)\n",
    "]\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=representation,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_energy, forces],\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "output_energy = spk.task.ModelOutput(\n",
    "    name=\"energy\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1,\n",
    "    metrics={\"MAE\": torchmetrics.MeanAbsoluteError()}\n",
    ")\n",
    "output_forces = spk.task.ModelOutput(\n",
    "    name=\"forces\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1,\n",
    "    metrics={\"MAE\": torchmetrics.MeanAbsoluteError()}\n",
    ")\n",
    "\n",
    "scheduler_args = {\n",
    "    \"factor\": 0.8,\n",
    "    \"patience\": 10,\n",
    "    \"min_lr\": 1e-06\n",
    "}\n",
    "\n",
    "task = AtomisticTaskFixed(\n",
    "    model_name=\"PaiNN\",\n",
    "    model=nnpot,\n",
    "    outputs=[output_energy, output_forces],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": 1e-4},\n",
    "    scheduler_cls=ReduceLROnPlateau,\n",
    "    scheduler_args=scheduler_args,\n",
    "    scheduler_monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "# create trainer\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=workpath)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=f\"{workpath}/checkpoints\",\n",
    "    filename=\"Painn-{epoch:03d}_{val_loss:4f}\"\n",
    ")\n",
    "callbacks = [\n",
    "    lr_monitor,\n",
    "    checkpoint_callback\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=devices,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=workpath,\n",
    "    max_epochs=nepochs,\n",
    ")\n",
    "\n",
    "trainer.fit(task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3e51125-0334-4db5-9774-14b2c84bc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f30f4",
   "metadata": {},
   "source": [
    "## Initializing the testing procedure and computing the metric's result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cfe5514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:root:Split file was given, but `num_train (-1) != len(train_idx)` (0)!\n",
      "WARNING:root:Split file was given, but `num_val (-1) != len(val_idx)` (0)!\n",
      "Restoring states from the checkpoint path at /mnt/2tb/ber/nablaDFT/examples/logs/checkpoints/Painn-epoch=001_val_loss=0.023082.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at /mnt/2tb/ber/nablaDFT/examples/logs/checkpoints/Painn-epoch=001_val_loss=0.023082.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f522c2d8acfc4081b41573eb7fdc1199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2tb/ber/miniconda3/envs/nablaDFT/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 100. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/mnt/2tb/ber/miniconda3/envs/nablaDFT/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 74. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_energy_MAE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     6.309358596801758     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_forces_MAE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.06932030618190765    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     40.81146240234375     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_energy_MAE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    6.309358596801758    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_forces_MAE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.06932030618190765   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    40.81146240234375    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 40.81146240234375,\n",
       "  'test_energy_MAE': 6.309358596801758,\n",
       "  'test_forces_MAE': 0.06932030618190765}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "cutoff = 5.0\n",
    "gpu = 0\n",
    "\n",
    "if gpu == -1:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(f\"cuda:{gpu}\")\n",
    "\n",
    "datamodule_test = ASENablaDFT(\n",
    "    \"test\",\n",
    "    dataset_name=\"dataset_test_conformations_2k\",\n",
    "    datapath=\"database_test\",\n",
    "    data_workdir=logspath,\n",
    "    batch_size=batch_size,\n",
    "    train_ratio=0.0,\n",
    "    val_ratio=0.0,\n",
    "    test_ratio=1.0,\n",
    "    num_workers=4,\n",
    "    test_transforms=[\n",
    "        trn.ASENeighborList(cutoff=cutoff),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    splitting=TestSplit()\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=devices,\n",
    "    default_root_dir=workpath,\n",
    "    inference_mode=False\n",
    ")\n",
    "\n",
    "trainer.test(model=task, datamodule=datamodule_test, ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488c51c",
   "metadata": {},
   "source": [
    "## Downloading dataset with atomic energies subtracted and starting the training process. remove???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "logspath = 'logs_remove_atomrefs'\n",
    "workpath = logspath\n",
    "data_remove_atomrefs = NablaDFT(\"ASE\", dataset_name,\n",
    "                datapath=datapath,\n",
    "                data_workdir=workpath,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=4,\n",
    "                transforms=[\n",
    "                            trn.ASENeighborList(cutoff=cutoff),\n",
    "                            trn.RemoveOffsets(\"energy\", remove_mean=True, remove_atomrefs=True),\n",
    "                            trn.CastTo32()\n",
    "                           ],\n",
    "                split_file=os.path.join(workpath, \"split.npz\"))\n",
    "\n",
    "pairwise_distance = spk.atomistic.PairwiseDistances()\n",
    "radial_basis = spk.nn.radial.GaussianRBF(\n",
    "    n_rbf=n_rbf,\n",
    "    cutoff=cutoff\n",
    ")\n",
    "cutoff_fn = spk.nn.cutoff.CosineCutoff(cutoff)\n",
    "representation = rep.PaiNN(\n",
    "    n_interactions=n_interactions,\n",
    "    n_atom_basis=n_atom_basis,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=cutoff_fn\n",
    ")\n",
    "pred_energy = spk.atomistic.Atomwise(\n",
    "    n_in=representation.n_atom_basis,\n",
    "    output_key=\"energy\"\n",
    ")\n",
    "postprocessors = [\n",
    "    trn.CastTo64(),\n",
    "    trn.AddOffsets(\"energy\", add_mean=True,\n",
    "    add_atomrefs=True)\n",
    "]\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=representation,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_energy],\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "output_energy = spk.task.ModelOutput(\n",
    "    name=\"energy\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1,\n",
    "    metrics={\"MAE\": torchmetrics.MeanAbsoluteError()}\n",
    ")\n",
    "\n",
    "scheduler_args = {\n",
    "    \"factor\": 0.8,\n",
    "    \"patience\": 10,\n",
    "    \"min_lr\": 1e-06\n",
    "}\n",
    "\n",
    "task = spk.task.AtomisticTask(\n",
    "    model=nnpot,\n",
    "    outputs=[output_energy],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": 1e-4},\n",
    "    scheduler_cls=ReduceLROnPlateau,\n",
    "    scheduler_args=scheduler_args,\n",
    "    scheduler_monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "# create trainer\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=workpath)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        model_path=os.path.join(workpath, \"best_inference_model\"),\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=devices,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=workpath,\n",
    "    max_epochs=nepochs,\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"step\",\n",
    "    mode=\"max\",\n",
    "    filename=\"last-chkpt-{epoch:02d}-{global_step}\",\n",
    ")\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        model_path=os.path.join(workpath, \"best_inference_model\"),\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    ),\n",
    "    lr_monitor,\n",
    "    checkpoint_callback\n",
    "]\n",
    "\n",
    "trainer.fit(task, datamodule=data_remove_atomrefs.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3780fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datapath = 'database_test'\n",
    "batch_size = 100\n",
    "cutoff = 5.0\n",
    "gpu = 0\n",
    "\n",
    "if gpu == -1:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(f\"cuda:{gpu}\")\n",
    "\n",
    "data_remove_atomrefs_test = NablaDFT(\n",
    "    \"ASE\",\n",
    "    dataset_name_test,\n",
    "    datapath=datapath,\n",
    "    data_workdir=workpath,\n",
    "    distance_unit=\"Bohr\",\n",
    "    batch_size=batch_size,\n",
    "    train_ratio=0,\n",
    "    num_workers=1,\n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=cutoff),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    split_file=os.path.join(workpath, \"split_test.npz\")\n",
    ")\n",
    "\n",
    "data_remove_atomrefs_test.dataset.prepare_data()\n",
    "data_remove_atomrefs_test.dataset.setup()\n",
    "\n",
    "best_model = torch.load(os.path.join(workpath, 'best_inference_model'))\n",
    "best_model = best_model.cuda()\n",
    "best_model = best_model.eval()\n",
    "\n",
    "metric = torchmetrics.MeanAbsoluteError()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in tqdm.tqdm(data_remove_atomrefs_test.dataset.val_dataloader()):\n",
    "        for k in x:\n",
    "            if x[k].dtype == torch.float64:\n",
    "                x[k] = x[k].float()\n",
    "            x[k] = x[k].to(device)\n",
    "\n",
    "        target = x['energy'].cpu().clone()\n",
    "        prediction = best_model(x)['energy'].cpu()\n",
    "        mae = metric(prediction, target)\n",
    "\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030624f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
